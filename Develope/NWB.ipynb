{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root pynwb.file.NWBFile at 0x2234826980752\n",
      "Fields:\n",
      "  acquisition: {\n",
      "    array_data <class 'pynwb.base.TimeSeries'>,\n",
      "    image_data <class 'pynwb.base.TimeSeries'>,\n",
      "    int_data <class 'pynwb.base.TimeSeries'>,\n",
      "    list_data <class 'pynwb.base.TimeSeries'>,\n",
      "    string_data <class 'pynwb.base.TimeSeries'>\n",
      "  }\n",
      "  file_create_date: [datetime.datetime(2025, 1, 16, 11, 52, 40, 824430, tzinfo=tzoffset(None, 7200))]\n",
      "  identifier: example-id\n",
      "  session_description: Python object storage\n",
      "  session_start_time: 2025-01-16 09:52:40.824430+00:00\n",
      "  timestamps_reference_time: 2025-01-16 09:52:40.824430+00:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reads an NWB file and prints its contents\n",
    "from pynwb import NWBHDF5IO\n",
    "import os\n",
    "\n",
    "file_name = \"example_file.nwb\"\n",
    "folder_path = \"Test\"\n",
    "\n",
    "# Create full file path\n",
    "full_file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "with NWBHDF5IO(full_file_path, \"r\") as io:\n",
    "    nwbfile = io.read()\n",
    "    print(nwbfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new NWB file\n",
    "# create nwb file and save initial skeleton to disk\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create \n",
    "TimeSeries - units spikes times\n",
    "ImageSeries - unique images presented using the projector\n",
    "DynamicTable - time of image presentation and link to index of the image presented\n",
    "\n",
    "acquisition: raw data acquired from the experimental subject\n",
    "   units: spike times of individual neurons\n",
    "   stimulus: time series data that represents a optogenetic stimulus time series\n",
    "\n",
    "\"\"\"\n",
    "from pynwb import NWBFile, NWBHDF5IO\n",
    "from pynwb.image import GrayscaleImage, Images\n",
    "from pynwb.core import DynamicTable\n",
    "from pynwb.file import Subject\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import numpy as np\n",
    "\n",
    "culture_number = 1 # unique identifier for the culture\n",
    "culture_DIV = 14 # days in vitro of the culture\n",
    "culture_id = f'culture_{culture_number}_DIV{culture_DIV}'\n",
    "\n",
    "subject = Subject(\n",
    "\n",
    "    subject_id=culture_id,\n",
    "    description=\"Primary cortical neuron culture from E18 rat embryos\",\n",
    "    species=\"Rattus norvegicus\",\n",
    "    genotype=\"WT\",\n",
    "    age=\"E18\",  # Embryonic day 18 at dissection\n",
    "    plating_date = datetime(2025, 3, 12),\n",
    "    strain=\"c57BL/6\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Create NWB file\n",
    "nwbfile = NWBFile(\n",
    "    session_description='Optogenetic stimulation with static image stimuli',\n",
    "    identifier='OPTO_IMG_SESSION_STATIC_ARRAY',\n",
    "    session_start_time=datetime.now(timezone.utc)\n",
    ")\n",
    "\n",
    "# --- Properly store static images using Images container ---\n",
    "unique_images_data = np.random.randint(0, 256, (11, 256, 256), dtype=np.uint8)\n",
    "\n",
    "# Convert numpy arrays into NWB GrayscaleImage objects\n",
    "image_objs = []\n",
    "for idx, img_data in enumerate(unique_images_data):\n",
    "    img = GrayscaleImage(name=f'img_{idx}', data=img_data, description=f'Opto stim image {idx}')\n",
    "    image_objs.append(img)\n",
    "\n",
    "# Store the GrayscaleImages in an Images container\n",
    "image_container = Images(\n",
    "    name='stimulus_image_library',\n",
    "    images=image_objs,\n",
    "    description='Static image library used for optogenetic stimulation'\n",
    ")\n",
    "\n",
    "# Explicitly add the static images to the stimulus group\n",
    "nwbfile.add_stimulus(image_container)\n",
    "#nwbfile.stimulus['stimulus_image_library'] = image_container\n",
    "\n",
    "# processing/stimulus_presentation____________________________________________________________________________________\n",
    "# --- Create a DynamicTable referencing the image indices ---\n",
    "presentation_table = DynamicTable(\n",
    "    name='image_time_index',\n",
    "    description='Table recording presentation times of indiced images referencing the static image library'\n",
    ")\n",
    "presentation_table.add_column('start_time', 'Time at which image was presented')\n",
    "presentation_table.add_column('image_index', 'Index of the presented image')\n",
    "\n",
    "# Populate the presentation table\n",
    "for i in range(100):\n",
    "    img_idx = np.random.randint(0, 10)\n",
    "    start_time = i * 0.5\n",
    "    presentation_table.add_row(start_time=start_time, image_index=img_idx)\n",
    "\n",
    "# Store the presentation metadata clearly in the processing module\n",
    "proc_module = nwbfile.create_processing_module(\n",
    "    name='stimulus_index_time',\n",
    "    description='Stimulus timing data referencing static stimulus images'\n",
    ")\n",
    "proc_module.add_data_interface(presentation_table)\n",
    "\n",
    "# Write NWB file\n",
    "with NWBHDF5IO('optical_stim_session_static_array2.nwb', 'w') as io:\n",
    "    io.write(nwbfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acquisition\n",
      "analysis\n",
      "general\n",
      "processing\n",
      "processing/stimulus_index_time\n",
      "processing/stimulus_index_time/image_time_index\n",
      "specifications\n",
      "specifications/core\n",
      "specifications/core/2.7.0\n",
      "specifications/hdmf-common\n",
      "specifications/hdmf-common/1.8.0\n",
      "specifications/hdmf-experimental\n",
      "specifications/hdmf-experimental/0.5.0\n",
      "specifications/ndx-grayscalevolume\n",
      "specifications/ndx-grayscalevolume/0.0.2\n",
      "specifications/ndx-icephys-meta\n",
      "specifications/ndx-icephys-meta/0.1.0\n",
      "specifications/ndx-spectrum\n",
      "specifications/ndx-spectrum/0.2.2\n",
      "stimulus\n",
      "stimulus/presentation\n",
      "stimulus/presentation/stimulus_image_library\n",
      "stimulus/templates\n",
      "    start_time  image_index\n",
      "id                         \n",
      "0          0.0            0\n",
      "1          0.5            6\n",
      "2          1.0            9\n",
      "3          1.5            1\n",
      "4          2.0            5\n",
      "We have  100  samples in the table\n"
     ]
    }
   ],
   "source": [
    "# open the above NWB file\n",
    "#\"G:\\My Drive\\Research\\Projects\\Theory of cortical mind\\Object representation\\Software\\Python\\QT_GUI\\Develope\\optical_stim_session_static_array.nwb\"\n",
    "from pynwb import NWBHDF5IO\n",
    "import os\n",
    "\n",
    "\n",
    "from pynwb import NWBHDF5IO\n",
    "from nwbwidgets import nwb2widget\n",
    "import h5py\n",
    "\n",
    "file_name = \"optical_stim_session_static_array2.nwb\"\n",
    "folder_path = \"G:\\My Drive\\Research\\Projects\\Theory of cortical mind\\Object representation\\Software\\Python\\QT_GUI\\Develope\"\n",
    "# import the file\n",
    "full_file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "\n",
    "with h5py.File(full_file_path, 'r') as f:\n",
    "    # This function will be called for every object in the file.\n",
    "    def print_group(name, obj):\n",
    "        if isinstance(obj, h5py.Group):\n",
    "            print(name)\n",
    "\n",
    "    f.visititems(print_group)\n",
    "\n",
    "\n",
    "with NWBHDF5IO(full_file_path, 'r') as io:\n",
    "    nwbfile = io.read()\n",
    "\n",
    "    # Access the presentation_data processing module\n",
    "    proc_module = nwbfile.processing['stimulus_index_time']\n",
    "\n",
    "    # Access your dynamic table within the processing module\n",
    "    presentation_table = proc_module['image_time_index']\n",
    "\n",
    "    # Convert to DataFrame for easy viewing\n",
    "    df = presentation_table.to_dataframe()\n",
    "    print(df.head())\n",
    "    # number of samples in the table\n",
    "    print(\"We have \",len(df), \" samples in the table\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# use the nwbwidgets to visualize the data\n",
    "from nwbwidgets import nwb2widget\n",
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "file_name = \"optical_stim_session_static_array2.nwb\"\n",
    "#folder_path = \"G:\\My Drive\\Research\\Projects\\Theory of cortical mind\\Object representation\\Software\\Python\\QT_GUI\\Develope\"\n",
    "\n",
    "nwb = NWBHDF5IO(file_name, 'r').read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa5414e4dc44ba08c32420086b629b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='session_description:', layout=Layout(max_height='40px', max_width='â€¦"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwb2widget(nwb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject: <class 'dict'>\n",
      "stimulus: <class 'dict'>\n",
      "processing: <class 'dict'>\n",
      "acquisition: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# load the nwb file and extract the variables back to the original format\n",
    "\n",
    "from pynwb import NWBHDF5IO\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_group_data(group, group_dict):\n",
    "    if hasattr(group, 'keys'):\n",
    "        for key in group.keys():\n",
    "            value = group[key]\n",
    "            if hasattr(value, 'data'):\n",
    "                group_dict[key] = np.array(value.data)\n",
    "            else:\n",
    "                subgroup_dict = {}\n",
    "                extract_group_data(value, subgroup_dict)\n",
    "                group_dict[key] = subgroup_dict\n",
    "    elif hasattr(group, '__iter__') and not isinstance(group, str):\n",
    "        for idx, item in enumerate(group):\n",
    "            subgroup_dict = {}\n",
    "            extract_group_data(item, subgroup_dict)\n",
    "            group_dict[f'item_{idx}'] = subgroup_dict\n",
    "    else:\n",
    "        group_dict['value'] = str(group)\n",
    "\n",
    "\n",
    "def extract_attributes(obj):\n",
    "    attrs = {}\n",
    "    for attr in dir(obj):\n",
    "        if not attr.startswith('_') and not callable(getattr(obj, attr)):\n",
    "            attrs[attr] = getattr(obj, attr)\n",
    "    return attrs\n",
    "\n",
    "\n",
    "def extract_nwb_file(filepath):\n",
    "    extracted_data = {}\n",
    "    with NWBHDF5IO(filepath, 'r') as io:\n",
    "        nwbfile = io.read()\n",
    "\n",
    "        # Extract subject information dynamically\n",
    "        extracted_data['subject'] = extract_attributes(nwbfile.subject)\n",
    "\n",
    "        # Extract stimulus data\n",
    "        extracted_data['stimulus'] = {}\n",
    "        extract_group_data(nwbfile.stimulus, extracted_data['stimulus'])\n",
    "\n",
    "        # Extract processing modules\n",
    "        extracted_data['processing'] = {}\n",
    "        for mod_name in nwbfile.processing:\n",
    "            mod = nwbfile.processing[mod_name]\n",
    "            extracted_data['processing'][mod_name] = {}\n",
    "            extract_group_data(mod.data_interfaces, extracted_data['processing'][mod_name])\n",
    "\n",
    "        # Extract acquisition data\n",
    "        extracted_data['acquisition'] = {}\n",
    "        extract_group_data(nwbfile.acquisition, extracted_data['acquisition'])\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "nwb_data = extract_nwb_file('G:\\My Drive\\Research\\Projects\\Theory of cortical mind\\Object representation\\Software\\Python\\QT_GUI\\Develope\\optical_stim_session_static_array2.nwb')\n",
    "\n",
    "# Inspect extracted data\n",
    "for key, val in nwb_data.items():\n",
    "    print(f'{key}: {type(val)}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "The MaxOneRecordingInterface has not yet been implemented for systems other than Linux.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Updated file path with the correct location and file name\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#file_path = r\"L:\\gogomatlab\\MATLAB_Exp2_ReadFiles\\MaxWell\\0YonData\\Trace_20250122_10_56_37.raw.h5\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgogomatlab\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMATLAB_Exp2_ReadFiles\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMaxWell\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m0YonData\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msync_culture_42DIV.raw.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m interface \u001b[38;5;241m=\u001b[39m \u001b[43mMaxOneRecordingInterface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Extract what metadata we can from the source files\u001b[39;00m\n\u001b[0;32m     14\u001b[0m metadata \u001b[38;5;241m=\u001b[39m interface\u001b[38;5;241m.\u001b[39mget_metadata()\n",
      "File \u001b[1;32mc:\\Users\\Pattern\\.conda\\envs\\pattern\\Lib\\site-packages\\neuroconv\\datainterfaces\\ecephys\\maxwell\\maxonedatainterface.py:73\u001b[0m, in \u001b[0;36mMaxOneRecordingInterface.__init__\u001b[1;34m(self, file_path, hdf5_plugin_path, download_plugin, verbose, es_key)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03mLoad and prepare data for MaxOne.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    The key of this ElectricalSeries in the metadata dictionary.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m system() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinux\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe MaxOneRecordingInterface has not yet been implemented for systems other than Linux.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m     )\n\u001b[0;32m     77\u001b[0m hdf5_plugin_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHDF5_PLUGIN_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     79\u001b[0m     hdf5_plugin_path \u001b[38;5;129;01mor\u001b[39;00m Path\u001b[38;5;241m.\u001b[39mhome() \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdf5_plugin_path_maxwell\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     80\u001b[0m )\n\u001b[0;32m     81\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHDF5_PLUGIN_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(hdf5_plugin_path)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: The MaxOneRecordingInterface has not yet been implemented for systems other than Linux."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "from pathlib import Path\n",
    "from neuroconv.datainterfaces import MaxOneRecordingInterface\n",
    "\n",
    "# Updated file path with the correct location and file name\n",
    "#file_path = r\"L:\\gogomatlab\\MATLAB_Exp2_ReadFiles\\MaxWell\\0YonData\\Trace_20250122_10_56_37.raw.h5\"\n",
    "file_path = r\"L:\\gogomatlab\\MATLAB_Exp2_ReadFiles\\MaxWell\\0YonData\\sync_culture_42DIV.raw.h5\"\n",
    "\n",
    "interface = MaxOneRecordingInterface(file_path=file_path, verbose=False)\n",
    "\n",
    "# Extract what metadata we can from the source files\n",
    "metadata = interface.get_metadata()\n",
    "\n",
    "# For data provenance we add the time zone information to the conversion\n",
    "session_start_time = datetime(2020, 1, 1, 12, 30, 0, tzinfo=ZoneInfo(\"US/Pacific\"))\n",
    "metadata[\"NWBFile\"].update(session_start_time=session_start_time)\n",
    "\n",
    "# Save the NWB file in the specified folder\n",
    "nwbfile_path = \"NWB convert\"\n",
    "interface.run_conversion(nwbfile_path=nwbfile_path, metadata=metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protocol_1.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "protocols_number = 1 # unique identifier for the protocol\n",
    "file_name = f\"protocol_{protocols_number}.pkl\"\n",
    "print(file_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
